\chapter{Linear models}

\section{Introduction}

\subsection{Example: Subject and object relative clauses}

As a concrete example, consider a classic question from the psycholinguistics literature: are subject relatives easier to process than object relatives? The data come from Experiment 1 in a well-known paper by Grodner and Gibson \cite{grodner}.

In two important papers, Gibson and colleagues \cite{gibson00} and \cite{grodner} argued that object relative clause sentences are more difficult to process than subject relative clause sentences because the distance between the relative clause verb \textit{sent} and the head noun phrase of the relative clause, \textit{reporter}, is longer in object vs subject relatives. Examples are shown below.

(1a) The reporter who the photographer sent to the editor was hoping for a good story. (object relative)

(1b) The reporter who sent the photographer to the editor was hoping for a good story. (subject relative)

The underlying explanation that Grodner and Gibson provided has to do with memory processes: shorter linguistic dependencies are easier to process due to either reduced interference or decay, or both. For implemented computational models that investigate closely related issues, see \cite{lewisvasishth:cogsci05,EngelmannJaegerVasishthSubmitted2018}.

In the Grodner and Gibson data, the dependent measure is reading time at the relative clause verb, in milliseconds. We are expecting longer reading times in object relatives sentences compared to subject relatives.

First, load and reformat the data:

<<echo=FALSE>>=
library(dplyr)
gg05e1 <- read.table("data/GrodnerGibson2005E1.csv",
                     sep=",", header=TRUE)
gge1 <- gg05e1 %>% filter(item != 0)

gge1 <- gge1 %>% mutate(word_positionnew = ifelse(item != 15 & word_position > 10, word_position-1, word_position)) 
#there is a mistake in the coding of word position,
#all items but 15 have regions 10 and higher coded
#as words 11 and higher

## get data from relative clause verb:
gge1crit <- subset(gge1, ( condition == "objgap" & word_position == 6 ) |
            ( condition == "subjgap" & word_position == 4 ))
@


Each subject is seeing 8 items per condition:

<<>>=
head(xtabs(~subject+condition,gge1crit))
@

That is, there are 16 items, and 42 subjects, and two conditions.

Isolate the relevant columns, and aggregate the data such that we have one set of paired data for each subject (instead of 8 pairs). (This aggregation is a common way to do data analysis in psycholinguistics. We will use a better method in the next chapter.)

<<>>=
dat<-gge1crit[,c(1,2,3,6)]
head(dat)
dat<-aggregate(data=gge1crit,rawRT~subject+
                           condition,
          FUN=mean)
head(dat)
@

\subsubsection{The simple linear model}

Suppose $y$ is a vector of continuous responses; assume for now that it is coming from a normal distribution. In other words, the random variable $Y$ has the pdf:

\begin{equation}
Y \sim Normal(\mu,\sigma)
\end{equation}

A crucial assumption here is that the vector of data $y$ is \textbf{independent and identically distributed}. This implies that if we have $n$ data points, $y_1,y_2,\dots,y_n$, each one of these data points has the same distribution, and is indepenent from each other. We write this assumption into the equation as follows:

\begin{equation}
Y \overset{iid}{\sim} Normal(\mu,\sigma)
\end{equation}



One can equivalently write the above model as a so-called linear model (I'm dropping the expression iid, as it's understood in this chapter from this point on):

\begin{equation}
y = \mu + \varepsilon \hbox{ where } \varepsilon \sim Normal(0,\sigma)
\end{equation}

The goal is to estimate two parameters, $\mu,\sigma$. 
Here, we will have the R function lm do the estimation.

<<>>=
m<-lm(rawRT~1,dat)
summary(m)
@

The Intercept shown above is an estimate of the grand mean from the data:

<<>>=
round(mean(dat$rawRT),2)
@

But this model is actually not the relevant one for these data: It doesn't answer our question, which is ``are SRs easier to process than ORs?''

To answer this question, we have to think about our data as containing not one vector of $y$ responses, but  two vectors. The first vector, of length $n_{obj}=42$, is:

\begin{equation}
y_{obj} \sim Normal(\mu_{obj},\sigma)
\end{equation}


and the second vector, also of length $n_{subj}=42$, is:

\begin{equation}
y_{subj} \sim Normal(\mu_{subj},\sigma)
\end{equation}

Thus, the vector $y_{obj}$ contains $n_{obj}$ data points: 
$y_{obj,1}, y_{obj,2},\dots, y_{obj,42}$. Similarly, the 
vector $y_{subj}$ contains $n_{subj}$ data points: 
$y_{subj,1}, y_{subj,2},\dots, y_{subj,42}$

It happens to be the case here that the data are \textbf{paired}: the first pair of data points $y_{obj,1}$ and  $y_{subj,1}$ comes from the same subject with id 1, the second set from some other subject with id 2, etc. You can check this by looking at the number of data points from each subject in each condition:

<<>>=
head(xtabs(~subject+condition,dat))
@

We want to know the average difference in reading times between the two conditions. Under the NHST way of thinking, we set up our null hypothesis to be that the difference in means is 0:

\begin{equation}
H_0: \mu_{obj} = \mu_{subj} \Leftrightarrow \mu_{obj} - \mu_{subj} = 0
\end{equation}

We could have reversed object and subject relatives' means above, the order is irrelevant.

Let us now consider the relative clause example.
First, let's plot the distributions of reading times by factor levels:


<<fig.width=5,fig.height=5>>=
OR<-subset(dat,condition == "objgap")$rawRT
SR<-subset(dat,condition == "subjgap")$rawRT

op <- par(mfrow=c(1,2),pty="s")
plot(density(OR),main="OR",xlab="")
plot(density(SR),main="SR",xlab="")
@

The empirical density plots shown above don't seem to show a normal distribution, as assumed in the equation we wrote above:

\begin{equation}
Y \sim Normal(\mu,\sigma)
\end{equation}

If anything, in each condition, the underlying pdf is more like a LogNormal:

\begin{equation}
Y \sim LogNormal(\mu,\sigma)
\end{equation}

Actually, it's even more complicated than that: it seems to be a mixture distribution. See \cite{VasishthMixture2017} for more discussion on this point. 

It is pretty common in psycholinguistics to ignore the normality violation of the modeling assumption! For now, we will also ignore it; later on, we will return to this point. 

Looking at the means by condition, it is clear that object relatives have a longer reading time at the relative clause verb \textit{sent}.

<<>>=
means<-round(with(dat,
                  tapply(rawRT,condition,mean)))
means
@

This is how one could do a t-test with such data, to compare means across (sets of) conditions. 

<<>>=
t.test(SR,OR,paired=TRUE)
@

This is the one-sample t-test we saw earlier, where we subtract each subject's OR reading time from that subject's SR reading time and get a vector of scores.

<<>>=
t.test(SR-OR)
@

For comparison, the published result in \cite{grodner} is:  t1(1, 41) = 11.9, p $<$ .001. The t1 refers to aggregation over items we did above. The published effect is larger because they deleted some extreme values. On p.\ 268, the authors write:


\begin{quote}
Reading times that differed from the mean of a condition and region by more than 3 SDs were omitted from analyses. This adjustment discarded 1.6\% of the data. 
\end{quote}

It's not clear from this text whether this is on the unaggregated data or the aggregated data. It cannot be on the aggregated data, because then 19 out of 42 data points would be considered extreme in subject relatives:

<<>>=
3*sd(OR)
3*sd(SR)
deletionOR<-ifelse(OR>3*sd(OR),"remove","keep")
deletionSR<-ifelse(SR>3*sd(SR),"remove","keep")
table(deletionOR)
table(deletionSR)
@

For now we ignore this discrepancy between the published result and our finding above in the t-test.

Next, we will fit a \textbf{linear model} to these data. First, create a data frame in so-called long format for doing the data analysis.

<<>>=
aggregated_data<-data.frame(
  subj=rep(1:42,2),
  cond=factor(rep(c("or","sr"),each=42)),
              rt=c(OR,SR))
@

<<>>=
m0<-lm(rt~cond,aggregated_data)
summary(m0)
@

This linear model fits the same model as the  \textbf{two-sample} t-test we saw earlier:

<<>>=
t.test(SR,OR,paired=FALSE)
@

Notice that the t-value is the same in the t-test and the linear model.

Since the two-sample t-test is the wrong model (this is paired data), the linear model is obviously also an incorrect model for the aggregated data. We don't want to treat the data as unpaired if they are paired. We will soon fit the correct model. For now, let's focus on understanding what the linear model gives us.

\subsubsection{Unpacking the linear model output}

First, look at the coefficients, which define the \textbf{intercept} and \textbf{slope} of the fitted line respectively:

<<>>=
## coefficients:
coef(m0)
@

Compare these coefficients with the means we computed above.

<<>>=
means
@

The mean for SRs is 369 ms, and the mean for ORs is 369+102=471.

The fitted linear model implies two generative processes for the two conditions:

ORs:

\begin{equation}
y_{obj} =  471 + \varepsilon \hbox{ where } \varepsilon \sim Normal(0,201)
\end{equation}

SRs:

\begin{equation}
y_{subj} =  471 - 102 +  \varepsilon \hbox{ where } \varepsilon \sim Normal(0,201)
\end{equation}

This is encoded in the data frame by coding rows containing object relatives as 0's and rows containing subject relatives as 1's. This is called \textbf{treatment contrast coding}. We return to this below.

One assumption the model makes is that the $\varepsilon$, which are called the residuals, are normally distributed: $\varepsilon \sim Normal(0,\sigma)$. We can check whether this assumption is met by examining the distribution of the residuals.
We can extract the residuals:

<<>>=
## residuals:
res.m0<-residuals(m0)
@


and plot them by comparing them to a standard normal distribution. If the residuals are approximately normal, you should see the data points along a straight line angled at 45 degrees. This is clearly not true in our data.

\begin{figure}[!htbp]
\centering
<<>>=
library(car)
qqPlot(res.m0)
@
\caption{Residuals of the model m0.}
\label{fig:residuals}
\end{figure}

\subsubsection{How the coefficient estimates are computed}

Underlyingly, R uses a design matrix or model matrix, which is being used by R to estimate the coefficients:

<<>>=
##
head(model.matrix(m0),n=7)
@

It is very useful to understand a little bit about what these parts of the linear model are.
Our linear model equation for the relative clause model m0 above is a system of equations. The single equation:

\begin{equation} \label{eq1a}
Y_i = \beta_{0} + \beta_{1}X_i + \epsilon_i 
\end{equation}

\noindent
can be expanded to:

\begin{equation} \label{matrixeq1}
 \begin{array}{ccccccc}
Y_1    & = & \beta_0 & + & X_1 \beta_1 & + & \epsilon_1 \\
Y_2    & = & \beta_0 & + & X_2 \beta_1 & + & \epsilon_2 \\
Y_3    & = & \beta_0  & + & X_3 \beta_1 & + & \epsilon_3 \\
Y_4    & = & \beta_0 & + & X_4 \beta_1 & + & \epsilon_4 \\
\vdots &   & \vdots  &   & \vdots      &   & \vdots  \\
Y_{84}    & = & \beta_0 & + & X_{84} \beta_1 & + & \epsilon_{84} \\
\end{array} 
\end{equation}

The total number of rows $n$ here is 42*2=84 in the Grodner and Gibson data. This is because in the aggregated data we created above, each subject delivers two data points, one for SRs and one for ORs, and there are 42 subjects.

This system of linear equations can be restated in compact matrix form:

\begin{equation}
\mathbf{Y} = \mathbf{X} \mathbf{\beta} + \mathbf{\epsilon}
\end{equation}



\noindent
where

\textbf{Vector of responses}:

\begin{equation} \label{matrixsum}
\mathbf{Y} = \left( \begin{array}{c}
Y_1 \\
Y_2 \\
Y_3 \\
Y_4 \\
\vdots \\
Y_n \\
\end{array} \right)
\end{equation}

\textbf{The design matrix (in R this is called the model matrix)}:

\begin{equation} \label{matrixsum}
\mathbf{X} = \left( \begin{array}{cc}
1 & X_1 \\
1 & X_2 \\
1 & X_3 \\
1 & X_4 \\
\vdots & \vdots \\
1 & X_n \\
\end{array} \right)
\end{equation}

\textbf{Vector of coefficient parameters to be estimated}:

\begin{equation} \label{matrixsum}
\mathbf{\beta} = \left( \begin{array}{c}
\beta_0 \\
\beta_1 \\
\end{array} \right)
\end{equation}

and 

\textbf{Vector of error terms (residuals)}:

\begin{equation} \label{matrixsum}
\mathbf{\epsilon} = \left( \begin{array}{c}
\epsilon_1 \\
\epsilon_2 \\
\epsilon_3 \\
\epsilon_4 \\
\vdots \\
\epsilon_n \\
\end{array} \right)
\end{equation}

We could write the whole equation as:

\begin{equation} \label{matrixsum}
\left( \begin{array}{c}
Y_1 \\
Y_2 \\
Y_3 \\
Y_4 \\
\vdots \\
Y_n \\
\end{array} \right)
=
\left( \begin{array}{cc}
1 & X_1 \\
1 & X_2 \\
1 & X_3 \\
1 & X_4 \\
\vdots & \vdots \\
1 & X_n \\
\end{array} \right) 
\times 
\left( \begin{array}{c}
\beta_1 \\
\beta_2 \\
\end{array} \right)
+
\left( \begin{array}{c}
\epsilon_1 \\
\epsilon_2 \\
\epsilon_3 \\
\epsilon_4 \\
\vdots \\
\epsilon_n \\
\end{array} \right)
\end{equation}

Our principal goal when we fit a linear model is to find estimates of the parameters $\beta_0$ and $\beta_1$, the intercept and slope respectively; we will call the estimates from the data $\hat{\beta}_0$ and $\hat{\beta}_1$, in order to distinguish them from the unknown population parameters $\beta_0$ and $\beta_1$. This can be done by ``solving'' for the beta's using linear algebra. X is the model matrix, and X$'$ is the transpose of the model matrix. Y is the vector of dependent variables.

\begin{equation}
\beta=(X'X)^{-1} X'Y 
\end{equation}

You do not need to know how the above equation comes about; all you need to know is that given X, and Y, we can estimate the parameters.
In case you are interested in more details, please see my lecture notes on Linear Modeling: https://osf.io/ces89/.


Let's focus now on the vector $X_1,\dots,X_n$ in the model matrix. The first 42 values are 0 and the last 42 values are 1. This is because of the treatment contrast coding, which we examine in more detail next.

\subsubsection{Contrast coding}

Look at how each level of the relative clause conditions is converted into a numeric value:

<<>>=
contrasts(aggregated_data$cond)
@


The above contrast coding says the following: code object relatives as 0 and subject relatives as 1. 
Alphabetic order of the factor levels determines which level gets the 0 value. You can switch the order of the coding manually by doing the following:

<<>>=
aggregated_data$cond<-factor(aggregated_data$cond,levels=c("sr","or"))
@

Another way, which we will use in future, is to just create a new vector of 0,1 values using ifelse:

<<>>=
aggregated_data$c_trtmt<-ifelse(aggregated_data$cond=="sr",0,1)
@

As mentioned above, this kind of contrast is called treatment contrast coding.  

An alternative contrast coding is called sum contrasts: under this coding, object relatives are coded as 1 and subject relatives as -1 (the coding is arbitrary; we could have had it the other way around).

<<>>=
contrasts(aggregated_data$cond)<-contr.sum(2)
contrasts(aggregated_data$cond)
@

We can also define such a contrast coding by creating a new vector:

<<>>=
aggregated_data$c_sum<-ifelse(aggregated_data$cond=="sr",-1,1)
@


Now, the model looks like this:

<<>>=
m2<-lm(rt~c_sum,aggregated_data)
summary(m2)
@

The intercept is now the grand mean:

<<>>=
round(mean(aggregated_data$rt),2)
@

The difference between the reading times can be computed from the slope as follows:

<<>>=
(ORmeans<-420.21726+51.14286) 
(SRmeans<-420.21726-51.14286) 
## difference in means:
SRmeans-ORmeans
@

As mentioned above, in future, we will be using not the treatment contrast coding but this sum contrast coding.

\subsection{True discovery rate (power)}

Even before we had collected the data, we should have done a power analysis. We should make sure that we have adequate power before we run the experiment, otherwise the logic of NHST doesn't work, due to Type M and S errors. We can check prospective power \textit{for a future study} as follows:

In the published paper, the difference between object and subject relative reading times was 65 ms. If that's the true value of the effect, then we could compute power as follows, assuming a standard deviation of 213. This sd is estimated from the Grodner and Gibson data as follows:

The one-sample t-test gave us a t-value of -3.1, and the difference in means is -102. Sample size is 42. We know from the t-test that:

\begin{equation}
t= -3.1=\frac{-102}{s/\sqrt{42}}
\end{equation}

So just solve for s (exercise). The answer is approximately 213. 

<<>>=
power.t.test(d=65,sd=213,type="one.sample",n=42)
@

The above is an analytical solution that R provides, based on closed-form formulas that you can find in statistics textbooks.

50\% power is too low. How many subjects should we have collected data from if we wanted 80\% power?

<<>>=
power.t.test(d=65,sd=213,power=0.80,type="one.sample")
@

The answer is 87 subjects at a minimum. 

In such a situation, I would also ask: suppose the true effect is not 65 ms but 30 ms. We know that under low power conditions, any significant effect we find is guaranteed to be an overestimate. So Grodner and Gibson's estimate is probably biased upwards (Type M error) if their power is this low. 

How many subjects would we need to obtain 80\% power, if the true effect was as low as 30 ms? The answer may shock you:

<<>>=
power.t.test(d=30,sd=213,power=0.80,type="one.sample")
@

The reality is that small effects in areas like sentence processing are quite common. All the big and robust effects have already been found.
Studies tend to be heavily underpowered, leading to exaggerated estimates getting published. See my papers \cite{VasishthMertzenJaegerGelman2018,JaegerEngelmannVasishth2017} for extended dicussions about this point in the context of sentence processing.

When doing such power calculations, one can also compute power curve for different effect sizes, given a sample size:

<<>>=
effect<-seq(10,65,by=0.1)
pow<-rep(NA,length(effect))

for(i in 1:length(effect)){
pow[i]<-power.t.test(d=effect[i],sd=213,n=42,
             type="one.sample")$power
}  
plot(effect,pow,type="l",
     main="power curve (n=42,sigma=213)",
     xlab="effect (ms)",ylab="power")
@

This concludes our introduction to linear models when we have an experiment with a factorial design (here, two conditions).
Next, we consider the case where we have a continuous predictor.

\section{A linear model with a continuous predictor}

In our relative clause example, the predictor is categorial. What about when we have continuous predictors. An example is  instructors' beauty levels measured on a continuous scale as predictors of their teaching evaluations? Beauty levels are centered; this means that a beauty level of 0 means average beauty level. This is a data set from a paper by 
Hamermesh and Parker
(Beauty in the Classroom: Instructors' Pulchritude and Putative Pedagogical Productivity," Economics of Education Review, August 2005). I got the data from \cite{gelmanhill07}. 




<<>>=
## Example with a continuous predictor:

## teacher's evaluations as a function of their beauty score:
bdata <- read.table("data/beauty.txt",header=TRUE)
head(bdata)
@

Let's first plot the data first:

<<>>=
plot(evaluation~beauty,bdata)
@


<<>>=
m3<-lm(evaluation~beauty,bdata)
@

<<>>=
summary(m3)
@

The intercept tells you the estimated mean teaching evaluation when the beauty level (the predictor) has value 0---which is the average ``beauty'' level among these professors. The slope tells you the estimated increase in teaching evaluation when beauty level is increased by 1 unit.

\subsection{Does beauty level affect evaluation?}

If you look at the p-value in model m3 above, it is clear that we can reject the null hypothesis that beauty level doesn't affect evaluation. But this doesn't mean the effect is real, and it doesn't mean there is a causal link between beauty and evaluations (there could be, it just isn't necessarily so). For one thing, the plot of evaluation against beauty looks very noisy: the points are widely spread out. 

And if power is low, the significant estimate is exaggerated. Suppose the true effect was half of 0.133. Power would be remarkably low:

(I got the sd in the calculation below by looking at the SE of the effect of beauty. $SE=0.03218=s/\sqrt{461}$. So, s=0.69.)

<<>>=
power.t.test(d=0.133/2,sd=0.69,n=461,type="two.sample")
@

Again, in a potentially low-power situation, we should not trust significant effects. In any case, whether the observed effect of 0.133 has any theoretical relevance would have to worked out through domain expertise. What does a unit increase in beauty mean? Is it even percievable? Does an increase of 0.133  for a unit increase of beauty mean anything important from the perspective of teaching evaluation (I would say no). These kinds of questions should be considered when evaluating an effect; just looking at the p-value is not enough.



\section{Using fake-data simulation to understand a model}

One effective way to understand how well your model represents the phenomenon we are modeling is by fake data simulation.

The beauty model expresses how we think the evaluations were generated. Of course there are other determinants of teaching quality than beauty, so it's not a realistic model. But what data would the model generate under repeated sampling? We can find this out.

First, we define a function that generates fake data, using estimates from our fitted model above. The sample size is as in the beauty data.

<<>>=
gen_fake<-function(n=463,intercept=4,
                   slope=0.133,
         predictor=bdata$beauty,
         sigma=0.69){
  eval<-intercept+slope*predictor+rnorm(n,mean=0,sd=0.69)
  fakedat<-data.frame(eval=eval,beauty=predictor)
  fakedat
  }
@

Then, we generate fake data 10000 times, and save the intercept and slope estimates, and the p-value for the effect of beauty.

<<>>=
nsim<-10000
savecoef<-matrix(rep(NA,nsim*2),ncol=2)
pvals<-rep(NA,nsim)
#op<-par(mfrow=c(4,5),pty="s")
for(i in 1:nsim){
  fakedat<-gen_fake()
  m<-lm(eval~beauty,fakedat)
 # plot(fakedat[,2],fakedat[,1],
  #     xlab="beauty",ylab="eval")
  #abline(m$coef,col="red")
  savecoef[i,]<-m$coef
  pvals[i]<-summary(m)$coefficient[2,4]
}
@

Now, we can ask some interesting questions:

\begin{enumerate}
\item If the effect were 0, how often would we get significant results? It should be about 5\% (Type I error).

<<>>=
nsim<-10000
savecoef<-matrix(rep(NA,nsim*2),ncol=2)
pvals<-rep(NA,nsim)
for(i in 1:nsim){
  fakedat<-gen_fake(slope=0)
  m<-lm(eval~beauty,fakedat)
  savecoef[i,]<-m$coef
  pvals[i]<-summary(m)$coefficient[2,4]
}
mean(pvals<0.05)
@

\item What is the distribution of the p-value when the true value of the slope is 0? Answer: the uniform distribution.

<<>>=
hist(pvals)
@
\item If the true value of the slope were 0.07 instead of 0.133, what would our power be?

<<>>=
nsim<-10000
savecoef<-matrix(rep(NA,nsim*2),ncol=2)
pvals<-rep(NA,nsim)
for(i in 1:nsim){
  fakedat<-gen_fake(slope=0.07)
  m<-lm(eval~beauty,fakedat)
  savecoef[i,]<-m$coef
  pvals[i]<-summary(m)$coefficient[2,4]
}
mean(pvals<0.05)
@

\item When power is as low as 40\%, how exaggerated will significant effects be?

<<>>=
hist(savecoef[,2][which(pvals<0.05)],xlab="estimate",freq=FALSE,main="Type M error with 40% power")
abline(v=0.07,col="red")
@

\end{enumerate}

Fake data simulation will prove very helpful in evaluating how realistic the model is, and whether we have any chance of detecting accurate estimates of an effect.

