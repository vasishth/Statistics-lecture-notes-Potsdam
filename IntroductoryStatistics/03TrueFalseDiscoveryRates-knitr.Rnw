\chapter{Calibrating the model's properties under repeated sampling}

Let's assume we do an experiment, compute the t-value and p-value for
our sample, and based on these values, reject the null hypothesis. As
we mentioned in the previous chapter, and as you can demonstrate to yourself
through simulated replication of experiments, due to the very nature
of random sampling it is always \emph{possible} to stumble on an unusual sample, one whose statistic just happens to be far from the population parameter. In this case it would, in fact, be an error to reject the hypothesis, though we wouldn't know it. The technical name for this is a \index{error, Type I}\textsc{Type I error}: the null hypothesis is true, but our sample incorrectly leads us to reject it.

The converse can also happen. Suppose the null hypothesis is indeed
false---there is a real difference between two population means, for
example---but the sample values we have happen to be so close to each other that this difference is not detectable. Here, the null hypothesis is false, but we fail to reject it based on our sample. Again, we have
been misled by the sampling process: this is known as a \index{error,
  Type II}\textsc{Type II error}.

In the first case, we would think our experiment had succeeded, publish our result, and move on, unaware of our mistake. Can we minimize this risk? In the second case, we don't get a significant difference, it appears our experiment has failed. Is there some way to minimize the risk? Our problems boil down to controlling \textbf{false discovery rate} (Type I error), and controlling \textbf{false non-discovery rate} (Type II error).

Addressing these questions is of fundamental importance in experimental design. It turns out that a couple of the obvious things we \emph{might} do to improve our experiments have unpleasant implications. For example, we might think that making the probability threshold more stringent---0.01 instead of 0.05, for example---will minimize the chance of error. We pay a price for that, however. The reason is that there is an intimate relationship between the two types of error, and a technique that simply aims to minimize one kind can unwittingly increase the chance of the other. This chapter uses simulation to explore this interaction.

\section{\index{Type I and Type II errors}Type I and Type II Errors}


We fix some conventions first for the text below. Let: $R$ = `Reject the null hypothesis $H_0$'; 
$\neg R$ = `Fail to reject the null hypothesis $H_0$.' These are \textbf{decisions} we make based on an experiment's outcome.

The decision $R$ or $\neg R$ is based on the sample.
Keep in mind that when we do an experiment we don't know whether the null
hypothesis is true or not.

The first step in attempting to minimize error is to have some way to measure it. It turns out we can use probability for this as well: we will use conditional probabilities of the following kind:
Let $P(R\mid H_0)$ = `Probability of rejecting the null
hypothesis \textit{conditional on the assumption that the null hypothesis is in fact true}.'

Let's work through the logical possibilities that could hold: the null
hypothesis could be in fact true or in fact false (but we don't know
which), and in addition our decision could be to accept or reject the
null hypothesis (see Table~\ref{nulltruefalse}). In only two of these
four possible cases do we make the right decision. In the table, think
of $\alpha$ as the threshold probability we have been using all along,
0.05.

\begin{table}
\centering
\caption{The logical possibilities given the two possible situations:
  null hypothesis true ($H_0$) or false  ($\neg H_0$).}
\label{nulltruefalse}       % Give a unique label
\begin{tabular}{p{5cm}p{3cm}p{3cm}}
\hline
Reality: & $H_0$ TRUE & $H_0$ FALSE \\
\hline
Decision from sample is `reject': & $\alpha$ & $1~-~\beta$ \\
                                     & \textbf{Type I error}                         & \textbf{Power} \\                                      
                                     & & \\
\hline
Decision from sample is `accept': & $1 - \alpha$ & $\beta$ \\
                                    &                                 & \textbf{Type II error}\\
\hline
\end{tabular}
\end{table}

As shown in Table~\ref{nulltruefalse}, the probability of a Type I
error $P(R\mid H_0)$ is $\alpha$, conventionally set at
0.05. We will see why this is so shortly. But it immediately follows
that the probability of the logical complement $P(\neg R\mid H_0)$ is
$1-\alpha$. We define the probability of a Type II error $P(\neg
R\mid\neg H_0)$ to be $\beta$ (more on this below), but it immediately
follows that the probability of the logical complement $P(\neg
R\mid\neg H_0) = 1 - \beta$. We call this probability
\textsc{power}. Thus, if we want to decrease the chance of a Type II
error, we need to increase the power of the statistical test.

Let's do some simulations to get a better understanding of these various
definitions. We focus on the case where the null hypothesis is in
fact 
false: there is a real difference between population means.

Assume a population with mean $\mu_1 = 60$, $\sigma_1 = 1$, and another with mean $\mu_2 = 62$, $\sigma_2 = 1$.
In this case we already \textit{know} that the null hypothesis is false.
The distribution corresponding to the null hypothesis is shown in Figure~\ref{fig:nullhyprejectionregion}. It is centered around 0, consistent with the null hypothesis that the difference between the means is 0.

We define a function for easily shading the regions of the plot we are
interested in. The function below, \texttt{shadenormal2}, is a
modified version of the function \texttt{shadenormal.fnc} available
from the package \texttt{languageR}  (you do not
need to load the library \texttt{languageR} to use the function below). 

First, we define a function that will plot Type I error
intervals. This function requires that several parameters be set (our
use of this function will clarify how to use these parameters):


<<>>=
## function for plotting type 1 error.
plot.type1.error<-function(x,
                           x.min,
                           x.max,
                           qnts,
                           mean,
                           sd,
                           gray.level,main,show.legend=TRUE){

        plot(x,dnorm(x,mean,sd), 
                     type = "l",xlab="",ylab="",main=main)
        abline(h = 0)

## left side    
    x1 = seq(x.min, qnorm(qnts[1]), qnts[1]/5)
    y1 = dnorm(x1, mean, sd)

    polygon(c(x1, rev(x1)), 
            c(rep(0, length(x1)), rev(y1)), 
            col = gray.level)

## right side            
    x1 = seq(qnorm(qnts[2]), x.max, qnts[1]/5)
    y1 = dnorm(x1, mean, sd)
    polygon(c(x1, rev(x1)), 
            c(rep(0, length(x1)), rev(y1)), 
            col = gray.level)
if(show.legend==TRUE){legend(2,0.3, legend="Type I error",fill=gray.level,cex=1)}
	}
@

Next, we define a function for plotting Type I and Type II errors;
this function additionally allows us to specify the mean of the null
hypothesis and the population mean that the sample is drawn from
(\texttt{mean.true}):

<<>>=
plot.type1type2.error<-function(x,
                           x.min,
                           x.max,
                           qnts,
                           mean.null,
                           mean.true,
                           sd,
                           gray1,
                           gray2,main,show.legend=TRUE){
    	## the reality:
    	plot(x, dnorm(x,mean.true,sd), 
    	     type = "l",ylab="",xlab="",main=main)
    	## null hypothesis distribution:
      lines(x,dnorm(x,mean.null,sd),col="black") 
      abline(h = 0)
    
    ## plot Type II error region: 
 
    	x1 = seq(qnorm(qnts[1]), x.max, qnts[1]/5)
        y1 = dnorm(x1, mean.true, sd) 
 
      polygon(c(x1, rev(x1)), 
              c(rep(0, length(x1)), 
              rev(y1)), col = gray2)

    ## plot Type I error region assuming alpha 0.05:

    x1 = seq(x.min, qnorm(qnts[1]), qnts[1]/5)
    y1 = dnorm(x1, mean.null, sd)
    polygon(c(x1, rev(x1)), c(rep(0, length(x1)), rev(y1)), col = gray1)

    x1 = seq(qnorm(qnts[2]), x.max, qnts[1]/5)
    y1 = dnorm(x1, mean.null, sd) ## changed
    polygon(c(x1, rev(x1)), c(rep(0, length(x1)), rev(y1)), col = gray1)

if(show.legend==TRUE){
    legend(2,0.3, legend=c("Type I error","Type II error"),
    fill=c(gray1,gray2),cex=1)}
}   
@

The above two functions are then used within another function,
\texttt{shadenormal2} (below), that plots either the Type I error probability region alone, or both Type I and Type II error probability regions. Playing with the parameter settings in this function allows us to examine the relationship between Type I and II errors.

<<>>=

shadenormal2<- 
function (plot.only.type1=TRUE,
          alpha=0.05,
          gray1=gray(0.3), ## type I shading
          gray2=gray(0.7), ## type II shading
          x.min=-6,
          x.max=abs(x.min),
          x = seq(x.min, x.max, 0.01),
          mean.null=0,
          mean.true=-2,
          sd=1,main="",show.legend=TRUE) 
{

    qnt.lower<-alpha/2
    qnt.upper<-1-qnt.lower
    qnts<-c(qnt.lower,qnt.upper)
 
    if(plot.only.type1==TRUE){

     plot.type1.error(x,x.min,x.max,qnts,mean.null,sd,
     gray1,main,show.legend)     

    } else { ## plot type I and type II error regions
      
   plot.type1type2.error(x,
                         x.min,
                         x.max,
                         qnts,
                         mean.null,
                         mean.true,
                         sd,
                         gray1,
                         gray2,main,show.legend)     
     }
}
@

<<label=nullhyprejectionregion,include=FALSE>>=
shadenormal2(plot.only.type1=TRUE)
@

\begin{figure}[!htbp]
  \centering
<<echo=FALSE>>=
<<nullhyprejectionregion>>
@
  \caption{The distribution corresponding to the null hypothesis,
    along with rejection regions (the Type I error probability region $\alpha$).}
  \label{fig:nullhyprejectionregion}
\end{figure}

The vertical lines in Figure~\ref{fig:nullhyprejectionregion}
represent the 95\% CI, and the shaded areas are the Type I error
regions for a two-sided t-test (with probability in the two regions
summing to $\alpha = 0.05$). A sample mean from one sample taken from
a population with mean zero could possibly lie in this region
(although it's unlikely, given the shape of the distribution), and
based on that one sample, we would incorrectly decide that the null
hypothesis is false when it is actually true. 

In the present example we \emph{know} there is a difference of
$-2$ between the population means. Let's plot the \emph{actual} (as opposed to hypothetical) sampling distribution of mean differences corresponding to this state of the world.

<<label=nullvstrue,include=FALSE>>=
shadenormal2(plot.only.type1=TRUE)
xvals <- seq(-6,6,.1)
lines(xvals,dnorm(xvals,mean=-2,sd=1),lwd=2)
@

\begin{figure}[!htbp]
\centering 
<<echo=FALSE>>=
<<nullvstrue>>
@ 
  \caption{The distribution corresponding to the null hypothesis and
    the distribution corresponding to the true population scores.}
  \label{fig:nullvstrue}
\end{figure}

Figure~\ref{fig:nullvstrue} shows the distribution corresponding to
the null hypothesis overlaid with the \emph{actual} distribution,
which we \emph{know} is centered around $-2$.  The vertical lines are again the
95\% CI, assuming the null hypothesis is true.

Now let's shade in the region that corresponds to Type II error; see Figure~\ref{fig:nullvstrue2}.
Notice that the values in this region lie \emph{within} the 95\% CI of the null hypothesis. To take a specific example, given that the population means really differ by $-2$, if in our particular sample the difference happened to be $-1$, we would fail to reject $H_0$ even though it is false. This is true for any value in this Type II error range.

<<label=nullvstrue2,include=FALSE>>=
shadenormal2(plot.only.type1=FALSE)
@

\begin{figure}[!htbp]
\centering 
<<echo=FALSE>>=
<<nullvstrue2>>
@
  \caption{The distribution corresponding to the true population
    scores along with the confidence intervals from the distribution
    corresponding to the null hypothesis.}
  \label{fig:nullvstrue2}
\end{figure}

Some important insights emerge from  Figure~\ref{fig:nullvstrue2}.  First, if the true
difference between the means had been not $-2$ but $-4$ (i.e., the
\index{effect size} \textsc{effect size} had been greater), then the Type II
error probability ($\beta$) will go down, and therefore power
($1-\beta$) will go up.  Let's confirm this visually (Figure~\ref{fig:nullvstrue3}). 

<<label=nullvstrue3,include=FALSE>>=
shadenormal2(plot.only.type1=FALSE,mean.true=-4)
@

\begin{figure}[!htbp]
\centering 
<<echo=FALSE>>=
<<nullvstrue3>>
@
  \caption{When the true difference, i.e., the effect size, increases
    from -2 to -4, Type II error probability decreases, and therefore
    power increases. Compare with Figure~\ref{fig:nullvstrue2}.}
  \label{fig:nullvstrue3}
\end{figure}

The second insight is that, for a fixed effect size, if we reduce $\alpha$, we also increase Type II error probability, which reduces power. Suppose $\alpha$ were 0.01; then the Type II error region would be as in Figure~\ref{fig:nullvstrue4}.


<<label=nullvstrue4,include=FALSE>>=
shadenormal2(plot.only.type1=FALSE,alpha=0.01,main="alpha=.01")
@

\begin{figure}[!htbp]
\centering 
<<echo=FALSE>>=
<<nullvstrue4>>
@
  \caption{When we decrease $\alpha$ from 0.05 to 0.01, Type II error probability increases, and therefore power decreases (compare Figure~\ref{fig:nullvstrue2}).}
  \label{fig:nullvstrue4}
\end{figure}

The third insight is that, for a given effect size, as we increase sample size, the 95\%
confidence intervals become tighter. This decreases Type II error
probability, and therefore increases power, as shown in Figure~\ref{fig:nullvstrue5}.

<<label=nullvstrue5,include=FALSE>>=
## simulating larger sample size by decreasing SD to 0.75 from 1:
shadenormal2(plot.only.type1=FALSE,sd=0.75,main="Larger sample size")
@

\begin{figure}[!htbp]
\centering 
<<echo=FALSE>>=
<<nullvstrue5>>
@ 
  \caption{Increasing sample size will tighten 95\% confidence intervals, decreasing Type II error probability, which increases power (compare with Figure~\ref{fig:nullvstrue2}).}
  \label{fig:nullvstrue5}
\end{figure}

To summarize, the best situation is when we have relatively high power
(low Type II error probability) and low Type I error probability
($\alpha$). By convention, we keep $\alpha$ at 0.05. We usually do not
want to change that: lowering $\alpha$ is costly in the sense that it
reduces power as well, as we just saw. What we do want to ensure is
that power is reasonably high; after all, why would you want to do an
experiment where you have only 50\% power or less? That would mean
that you have an a priori chance of finding a true effect (i.e., an
effect that is actually present in nature) only 50\% of the time or
less. As we just saw, we can increase power by increasing sample size,
and/or by increasing the sensitivity of our experimental design so that
we have larger effect sizes.

Researchers in psycholinguistics and other areas often do experiments
with low power (for logistical or other reasons); it is not unheard of
to publish reading studies (eyetracking or self-paced reading, etc.) or event-related potentials studies with 12 or 20 participants. When power is low, the frequentist method is dead on arrival: a null result tells you nothing if Type I error is high. 
On the other hand, a significant result is guaranteed to be 
exaggerated.  

We would like to make four observations here. These points are also discussed in detail in \cite{VasishthMertzenJaegerGelman2018}.

\begin{enumerate}
	\item
	At least in areas such as psycholinguistics, the null
        hypothesis is, strictly speaking, usually always false: When
        you compare reading times or any other dependent variable in
        two conditions, the a priori chance that the two means to be
        compared are \emph{exactly} identical is zero. The interesting
        question therefore is not whether the null hypothesis is
        false, but what counts as a null effect, and what the estimate of the true effect (the difference between the means) is. 
	\item
	One can in principle nearly always get a statistically
        significant effect given a large enough sample size; the
        question is whether the effect is large enough to be
        theoretically important and whether the difference between
        means has the expected sign. 
      \item Especially in areas like psycholinguistics, replication is
        not given the importance it deserves.  Note that we run a 5\%
        risk of declaring a significant difference when in fact there
        is none (or effectively none, see point 1 above). Replication is the only method to convince
        oneself that an effect is truly present. High power, reasonably large effect sizes, and
        actually replicated results should be your goal in
        experimental science.
A p-value of less than 0.05 does not tell you that you got a ``true effect''. Fisher himself pointed this out:

\begin{quote}
It is usual and convenient for experimenters to take-5 per cent. as a standard level of significance, in the sense that they are prepared to ignore all results which fail to reach this standard, and, by this means, to eliminate from further discussion the greater part of the fluctuations which chance causes have introduced into their experimental results. No such selection can eliminate the whole of the possible effects of chance. coincidence, and if we accept this convenient convention, and agree that an event which would occur by chance only once in 70 trials is decidedly ``significant," in the statistical sense, we thereby admit that \textbf{no isolated experiment, however significant in itself, can suffice for the experimental demonstration of any natural phenomenon; for the ``one chance in a million'' will undoubtedly occur, with no less and no more than its appropriate frequency}, however surprised we may be that it should occur to us. In order to assert that a natural phenomenon is experimentally demonstrable we need, not an isolated record, but a reliable method of procedure. In relation to the test of significance, we may say that a phenomenon is experimentally demonstrable when we know how to conduct an experiment which will rarely fail to give us a statistically significant result.
\end{quote}

\item
	Many researchers also believe that the lower the p-value, the lower the probability that the null hypothesis is true. However, as discussed earlier, this is a misunderstanding that stems from a
failure to attend to the meaning of the p-value.
\end{enumerate}

It follows from the above discussion that if you have a relatively narrow CI, and a nonsignificant result
($p > .05$), you have relatively high power and a relatively low
probability of making a Type II error (accepting the null hypothesis
as true when it is in fact false).
This is particularly important for interpreting null results (results where the p-value is greater than $0.05$).
\cite{hoenigheisey} suggest a heuristic: 
if you have a narrow CI, and a
nonsignificant result, you have some justification for concluding that
the null hypothesis may in fact be effectively true. Conversely, if you have a
wide CI and a nonsignificant result the result really is inconclusive.

\section{Computing sample size for a t-test using R}

Suppose you want to compute the sample size you need to have power 0.8 and alpha 0.05. I look at the paired sample case.

You need to decide on a few things:

\begin{itemize}
\item The magnitude of the effect you expect (based on previous work, or theory): delta (the difference between the means you're comparing)
\item  The standard deviation of the difference that between means that you expect (also based on previous data or theory)
\item Your significance level (alpha)
\item The power you want (APA recommends at least 0.80)
\item Type of t-test (usually a paired t-test in psycholinguistics)
\item The type of alternative hypothesis you have (we will always do the two-sided case)
\end{itemize}

<<>>=
power.t.test(n = NULL, delta = 100, sd = 100, sig.level = 0.05,
             power = 0.80,
             type = c("paired"),
             alternative = c("two.sided"),
             strict = FALSE)
@

The calculation suggests a sample size of 10 (rounding up). For other tests there are comparable functions in R, just look in the R help.

\section{The form of the power function}

Assume that the null hypothesis has mean 93, true sd is 5, and sample size is 20. If the null were true, the rejection region would be bounded by 

<<>>=
qnorm(0.025,mean=93,sd=5/sqrt(20))
qnorm(0.975,mean=93,sd=5/sqrt(20))
@

Intuitively, power will go up if the true value of the mean is far from the hypothesized mean, in either direction (greater than, or less than the hypothesized mean). We can look at how power changes as a function of how far away the true mean is from the hypothesized mean:


<<fig.width=5,fig.height=5>>=
sd<-5
n<-20
power.fn<-function(mu){
## lower and upper bounds of rejection regions
## defined by null hypothesis mean:
lower<-qnorm(0.025,mean=93,sd=5/sqrt(20))
upper<-qnorm(0.975,mean=93,sd=5/sqrt(20))

## lower rejection region:
z.l<-(lower-mu)/(sd/sqrt(n))	
## upper rejection region for given true mu:	
z.u<-(upper-mu)/(sd/sqrt(n))
## return rejection probability:
    return(pnorm(z.u,lower.tail=F)+
          pnorm(z.l,lower.tail=T))}

## a range of true values:	
alt<-seq(86,100,by=0.1)
pow<-power.fn(alt)
plot(alt,pow,type="l",
xlab="Specific parameters 
     for alternative hypothesis",
     ylab="Power",main="Power function 
     for H0: mu=93")
@

What do you think would be the shape of this function if you increased sample size? What would be the shape if you increased standard deviation?
First try to sketch the shapes on paper, and then use the above function to display the shape.

\section{Computing the power function (optional reading)}

You don't need to know this material, but I added it in case anyone is interested in finding out how exactly the power calculation is done.

As an example, let $H_0: \mu = 93$, and let $H_1: \mu \neq 93$. Assume that population sd $\sigma$ and sample size $n$ are given. Note that in realistic situations we don't know $\sigma$ but we can estimate it using $s$.

We can get a sample mean that is greater than $\mu$ or one that is smaller than $\mu$. Call these $\bar{x}_g$ and $\bar{x}_s$ respectively. 

In the case where we know $\sigma$, the test \textbf{under the null hypothesis} is:

\begin{equation}
\frac{\bar{x}_g-93}{\sigma / \sqrt{n}} > 1.96
\quad \hbox{ or }
\quad
\frac{\bar{x}_s-93}{\sigma / \sqrt{n}} > -1.96
\end{equation}

Solving for the two $\bar{x}$'s, we get:

\begin{equation}
\bar{x}_g > 1.96\frac{\sigma}{\sqrt{n}} + 93  
\quad \hbox{ or }
\quad
\bar{x}_s > -1.96\frac{\sigma}{\sqrt{n}} + 93  
\end{equation}

Now, power is the probability of rejecting the null hypothesis when the mean is whatever the alternative hypothesis mean is (some specific value $\mu$).

That, the test \textbf{under the alternative hypothesis} is:

\begin{equation}
\frac{\bar{x}_g - \mu}{\sigma / \sqrt{n}} > 1.96 
\quad
or 
\quad 
\frac{\bar{x}_s - \mu}{\sigma / \sqrt{n}} < -1.96 
\end{equation}

We can replace the $\bar{x}_g$ with its full form, and do the same with $\bar{x}_s$.

\begin{equation}
\frac{1.96\frac{\sigma}{\sqrt{n}} + 93 - \mu}{\sigma / \sqrt{n}} > 1.96 
\quad
or 
\quad 
\frac{-1.96\frac{\sigma}{\sqrt{n}} + 93 - \mu}{\sigma / \sqrt{n}} < -1.96 
\end{equation}

We can rewrite the above as:

\begin{equation}
\frac{1.96\frac{\sigma}{\sqrt{n}} - (\mu - 93)}{\sigma / \sqrt{n}} > 1.96 
\quad
or 
\quad 
\frac{-1.96\frac{\sigma}{\sqrt{n}} - (\mu - 93)}{\sigma / \sqrt{n}} < -1.96 
\end{equation}

Simplifying:

\begin{equation}
1.96 - \frac{(\mu - 93)}{\sigma / \sqrt{n}} > 1.96 
\quad
or 
\quad 
-1.96 - \frac{(\mu - 93)}{\sigma / \sqrt{n}} < -1.96 
\end{equation}

This is now easy to solve! We will use R's pnorm function in the equation below, which is the cumulative distribution function for the normal distribution. 
We can rewrite the above expression as:

\begin{equation}
[1 - pnorm(1.96 - \frac{(\mu - 93)}{\sigma / \sqrt{n}})] + pnorm(-1.96 - \frac{(\mu - 93)}{\sigma / \sqrt{n}})
\end{equation}

The above equation allows us to 

\begin{itemize}
\item
compute sample size for any given null (here 93) and alternative hypotheses, provided I have the population standard deviation.
\item 
compute power given a null and alternative hypothesis, population standard deviation, and sample size.
\end{itemize}

Example: 
suppose I need power of $0.99$ for
$H_0: \mu=93$ 
and $H_1: \mu=98$, $\sigma=5$. 

For this example, what sample size do I need? I take the above equation and fill in the values:

\begin{equation}
[1 - pnorm(1.96 - \frac{(98 - 93)}{5 / \sqrt{n}})] + pnorm(-1.96 - \frac{(98 - 93)}{5 / \sqrt{n}})
\end{equation}

Simplifying, this gives us:

\begin{equation}
[1 - pnorm(1.96 - \sqrt{n})] + pnorm(-1.96 - \sqrt{n})
\end{equation}

Note that the second term will be effectively zero for some reasonable n like 10:

<<>>=
pnorm(-1.96-sqrt(10))
@

So we can concentrate on the first term:

\begin{equation}
[1 - pnorm(1.96 - \sqrt{n})]
\end{equation}

If the above has to be equal to $0.99$, then 

\begin{equation}
pnorm(1.96 - \sqrt{n})=0.01
\end{equation}

So, we just need to find the value of the z-score that will give us a probability of approximately 0.01. You can do this analytically (exercise), but you could also play with some values of n to see what you get. The answer is $n=18$.

<<>>=
pnorm(1.96-sqrt(18))
@

\section{Stopping rules}

Psycholinguists and psychologists often adopt the following type of data-gathering procedure.
The experimenter gathers $n$ data points, then checks for significance ($p<0.05$ or not). If it's not significant, he gets more data ($n$ more data points). Since time and money are limited, he might decide to stop anyway at sample size, say, some multiple of $n$. 
One can play with different scenarios here. A typical $n$ might be $15$.

This approach would give us a range of p-values under repeated sampling. Theoretically, under the standard assumptions of frequentist methods, we expect a Type I error to be $0.05$. This is the case in standard analyses (I also track the t-statistic, in order to compare it with my stopping rule code below).

<<>>=
##Standard:
pvals<-NULL
tstat_standard<-NULL
n<-10
nsim<-1000
## assume a standard dev of 1:
stddev<-1
mn<-0
for(i in 1:nsim){
  samp<-rnorm(n,mean=mn,sd=stddev)
  pvals[i]<-t.test(samp)$p.value
  tstat_standard[i]<-t.test(samp)$statistic
}

## Type I error rate: about 5% as theory says:
table(pvals<0.05)[2]/nsim
@

But the situation quickly deteriorates as soon as adopt the strategy I outlined above. I will also track the distribution of the t-statistic below.

<<>>=
pvals<-NULL
tstat<-NULL
## how many subjects can I run?
upper_bound<-n*6

for(i in 1:nsim){
## at the outset we have no significant result:
  significant<-FALSE
## null hyp is going to be true,
## so any rejection is a mistake.
## take sample:
  x<-rnorm(n,mean=mn,sd=stddev)
while(!significant & length(x)<upper_bound){
  ## if not significant:
if(t.test(x)$p.value>0.05){
  ## get more data:
  x<-append(x,rnorm(n,mean=mn,sd=stddev))
  ## otherwise stop:
} else {significant<-TRUE}
}
## will be either significant or not:
pvals[i]<-t.test(x)$p.value
tstat[i]<-t.test(x)$statistic
}

## Type I error rate:
## much higher than 5%:
table(pvals<0.05)[2]/nsim
@

Now let's compare the distribution of the t-statistic in the standard case vs with the above stopping rule:

<<fig=T>>=
hist(tstat_standard,main="The t-distributions for the standard case (white) \n
     vs the stopping rule (gray)",freq=F)
hist(tstat,add=T,col="#0000ff22",freq=F)
@

We get fatter tails with the above stopping rule. Thus, this common strategy in psycholinguistics of running until significance is reached is at best a questionable research practice, and comes dangerously close to being classified as scientific misconduct.

One should fix one's sample size in advance based on a prospective  power analysis, not deploy a stopping rule like the one above; if we used such a stopping rule, we are much more likely to incorrectly declare a result as statistically significant.

Nevertheless, it is important not to unncessarily get subjects to do an experiment when the results are already clear. In order to collect subjects efficiently, one can decide on a data collection plan, deciding in advance how many times one will check for significance. \cite{pocock2013clinical} explains how to correct $\alpha$ for such a procedure. A better way (in my opinion) is to switch to Bayesian adaptive methods. See Experiment 7 in \cite{VasishthMertzenJaegerGelman2018} for an example.

%This strategy of p-value dependent stopping changes the distribution of the sampling distribution of the means.
%Crucially, the central limit theorem no longer applies.

%The t-test statistic you build from your experiment is distributed as:

%$
%\prod_{i=1}^{10m} \phi(x_i) \times %\prod_{j=1}^{m-1} I_{t(x_1,\ldots,x_{10j})>.05} \times
%I_{t(x_1,\ldots,x_{10j})<.05}
%$

%if $10m<60$ and from

%$
%\prod_{i=1}^{60} \phi(x_i) \times \prod_{j=1}^{5} I_{t(x_1,\ldots,x_{10j})>.05}
%$
%otherwise.

