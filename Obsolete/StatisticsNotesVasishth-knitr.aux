\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}What this course is about}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Quiz: Do you need this course?}{1}{section.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}How to survive and perhaps even enjoy this course}{4}{section.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Installing R and learning basic usage}{4}{section.1.3}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}The logic of (hypothetical) repeated sampling}{5}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Random variables}{5}{section.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Example of a discrete random variable}{5}{subsection.2.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Example of a continuous random variable}{5}{subsection.2.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}The normal distribution}{6}{section.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A histogram of the sample data.}}{7}{figure.2.1}}
\newlabel{hist1}{{2.1}{7}{A histogram of the sample data}{figure.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Plotting the normal distribution.}}{8}{figure.2.2}}
\newlabel{norm1}{{2.2}{8}{Plotting the normal distribution}{figure.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}The area under the curve in a normal distribution}{9}{section.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Two normal distributions with SD $=1$ (left), SD $=4$ (right). The lines delimit the region 2 SD from the mean in each case.}}{10}{figure.2.3}}
\newlabel{fig:normal2SD}{{2.3}{10}{Two normal distributions with SD $=1$ (left), SD $=4$ (right). The lines delimit the region 2 SD from the mean in each case}{figure.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Repeated sampling}{12}{section.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces The sampling distribution of the sample mean with 1000 samples of size 40.}}{13}{figure.2.4}}
\newlabel{fig:sdsmplot40}{{2.4}{13}{The sampling distribution of the sample mean with 1000 samples of size 40}{figure.2.4}{}}
\citation{kerns}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces The sampling distribution of the sample mean with samples of size 100.}}{14}{figure.2.5}}
\newlabel{fig:sdsmplot100}{{2.5}{14}{The sampling distribution of the sample mean with samples of size 100}{figure.2.5}{}}
\newlabel{sigmabar}{{2.6}{14}{Repeated sampling}{equation.2.4.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}The Central Limit Theorem}{15}{section.2.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces A sample from exponentially distributed population scores.}}{15}{figure.2.6}}
\newlabel{fig:exppopulation}{{2.6}{15}{A sample from exponentially distributed population scores}{figure.2.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces The sampling distribution of sample mean from an exponentially distributed population.}}{16}{figure.2.7}}
\newlabel{fig:exponentialsdsm}{{2.7}{16}{The sampling distribution of sample mean from an exponentially distributed population}{figure.2.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}$\sigma $ and $\sigma _{\mathaccentV {bar}016{x}}$}{16}{section.2.6}}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}The 95\% Confidence Interval for the Sample Mean}{17}{section.2.7}}
\@writefile{toc}{\contentsline {section}{\numberline {2.8}Realistic Statistical Inference}{18}{section.2.8}}
\@writefile{toc}{\contentsline {section}{\numberline {2.9}$s^2$ provides a good estimate of $\sigma ^2$}{18}{section.2.9}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces The distribution of the sample variance, sample size 40.}}{19}{figure.2.8}}
\newlabel{fig:varsample}{{2.8}{19}{The distribution of the sample variance, sample size 40}{figure.2.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces The sampling distribution of sample variances from an exponentially distributed population.}}{19}{figure.2.9}}
\newlabel{fig:varsampleexp}{{2.9}{19}{The sampling distribution of sample variances from an exponentially distributed population}{figure.2.9}{}}
\citation{kerns}
\@writefile{toc}{\contentsline {section}{\numberline {2.10}The t-distribution}{20}{section.2.10}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces A comparison between the normal (solid line) and t-distribution (broken line) for different degrees of freedom.}}{21}{figure.2.10}}
\newlabel{fig:tversusnorm}{{2.10}{21}{A comparison between the normal (solid line) and t-distribution (broken line) for different degrees of freedom}{figure.2.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.11}The One-sample t-test}{22}{section.2.11}}
\@writefile{toc}{\contentsline {section}{\numberline {2.12}Some Observations on Confidence Intervals}{23}{section.2.12}}
\newlabel{confidenceintervals}{{2.12}{23}{Some Observations on Confidence Intervals}{section.2.12}{}}
\newlabel{trickypoint}{{2.12}{23}{Some Observations on Confidence Intervals}{section.2.12}{}}
\citation{mooreetal}
\citation{hoekstra2014robust}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces A visualization of the proportion of cases where the population mean is contained in the 95\% CI, computed from repeated samples. The CIs that do not contain the population mean are marked with thicker lines.}}{25}{figure.2.11}}
\newlabel{repeatedCIsplot}{{2.11}{25}{A visualization of the proportion of cases where the population mean is contained in the 95\% CI, computed from repeated samples. The CIs that do not contain the population mean are marked with thicker lines}{figure.2.11}{}}
\citation{kerns}
\@writefile{toc}{\contentsline {section}{\numberline {2.13}Sample SD and Degrees of Freedom}{26}{section.2.13}}
\newlabel{ch3nminusone}{{2.13}{26}{Sample SD and \index {degrees of freedom}Degrees of Freedom}{section.2.13}{}}
\newlabel{nminusone}{{2.15}{26}{Sample SD and \index {degrees of freedom}Degrees of Freedom}{equation.2.13.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces The consequence of taking $n-1$ versus $n$ in the denominator for calculating variance, sample size 10.}}{27}{figure.2.12}}
\newlabel{fig:nminus1}{{2.12}{27}{The consequence of taking $n-1$ versus $n$ in the denominator for calculating variance, sample size 10}{figure.2.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.14}Summary of the Sampling Process}{27}{section.2.14}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces A summary of the notation used.}}{27}{table.2.1}}
\newlabel{summaryofnotation}{{2.1}{27}{A summary of the notation used}{table.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.15}Null Hypothesis Significance Tests}{28}{section.2.15}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces A sampling distribution with mean 70 and $\sigma _{\mathaccentV {bar}016{x}}$ = 1.206.}}{29}{figure.2.13}}
\newlabel{fig:nullhypexample}{{2.13}{29}{A sampling distribution with mean 70 and $\sigma _{\bar {x}}$ = 1.206}{figure.2.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.16}The Null Hypothesis}{29}{section.2.16}}
\@writefile{toc}{\contentsline {section}{\numberline {2.17}z-scores}{30}{section.2.17}}
\newlabel{zscores}{{2.17}{30}{z-scores}{section.2.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.18}P-values}{30}{section.2.18}}
\newlabel{pvals}{{2.18}{30}{P-values}{section.2.18}{}}
\newlabel{pvaluefallacy}{{2.18}{31}{P-values}{section.2.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces The sampling distribution of the sample mean (left) and its z-statistic (right).}}{32}{figure.2.14}}
\newlabel{fig:sampleMeanVsZ}{{2.14}{32}{The sampling distribution of the sample mean (left) and its z-statistic (right)}{figure.2.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.19}Hypothesis Testing: A More Realistic Scenario}{34}{section.2.19}}
\newlabel{ttestequation}{{2.23}{34}{Hypothesis Testing: A More Realistic Scenario}{equation.2.19.23}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces Hypothetical data showing reading times for adults and children.}}{36}{table.2.2}}
\newlabel{adultchilddata}{{2.2}{36}{Hypothetical data showing reading times for adults and children}{table.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.20}Comparing Two Samples}{36}{section.2.20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.20.1}$H_0$ in Two-sample Problems}{36}{subsection.2.20.1}}
\newlabel{twosampleproblemsnull}{{2.20.1}{36}{$H_0$ in \index {two sample problems}Two-sample Problems}{equation.2.20.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces The distribution of the difference of sample means of two samples. The vertical line shows the true difference between the means (30-7).}}{38}{figure.2.15}}
\citation{rice1995mathematical}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Calibrating the model's properties under repeated sampling}{41}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Type I and Type II Errors}{41}{section.3.1}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces The logical possibilities given the two possible situations: null hypothesis true ($H_0$) or false ($\neg H_0$).}}{42}{table.3.1}}
\newlabel{nulltruefalse}{{3.1}{42}{The logical possibilities given the two possible situations: null hypothesis true ($H_0$) or false ($\neg H_0$)}{table.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces The distribution corresponding to the null hypothesis, along with rejection regions (the Type I error probability region $\alpha $).}}{45}{figure.3.1}}
\newlabel{fig:nullhyprejectionregion}{{3.1}{45}{The distribution corresponding to the null hypothesis, along with rejection regions (the Type I error probability region $\alpha $)}{figure.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces The distribution corresponding to the null hypothesis and the distribution corresponding to the true population scores.}}{46}{figure.3.2}}
\newlabel{fig:nullvstrue}{{3.2}{46}{The distribution corresponding to the null hypothesis and the distribution corresponding to the true population scores}{figure.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces The distribution corresponding to the true population scores along with the confidence intervals from the distribution corresponding to the null hypothesis.}}{47}{figure.3.3}}
\newlabel{fig:nullvstrue2}{{3.3}{47}{The distribution corresponding to the true population scores along with the confidence intervals from the distribution corresponding to the null hypothesis}{figure.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces When the true difference, i.e., the effect size, increases from -2 to -4, Type II error probability decreases, and therefore power increases. Compare with Figure\nobreakspace  {}\ref  {fig:nullvstrue2}.}}{47}{figure.3.4}}
\newlabel{fig:nullvstrue3}{{3.4}{47}{When the true difference, i.e., the effect size, increases from -2 to -4, Type II error probability decreases, and therefore power increases. Compare with Figure~\ref {fig:nullvstrue2}}{figure.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces When we decrease $\alpha $ from 0.05 to 0.01, Type II error probability increases, and therefore power decreases (compare Figure\nobreakspace  {}\ref  {fig:nullvstrue2}).}}{48}{figure.3.5}}
\newlabel{fig:nullvstrue4}{{3.5}{48}{When we decrease $\alpha $ from 0.05 to 0.01, Type II error probability increases, and therefore power decreases (compare Figure~\ref {fig:nullvstrue2})}{figure.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Increasing sample size will tighten 95\% confidence intervals, decreasing Type II error probability, which increases power (compare with Figure\nobreakspace  {}\ref  {fig:nullvstrue2}).}}{48}{figure.3.6}}
\newlabel{fig:nullvstrue5}{{3.6}{48}{Increasing sample size will tighten 95\% confidence intervals, decreasing Type II error probability, which increases power (compare with Figure~\ref {fig:nullvstrue2})}{figure.3.6}{}}
\citation{VasishthMertzenJaegerGelman2018}
\citation{hoenigheisey}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Computing sample size for a t-test using R}{50}{section.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}The form of the power function}{50}{section.3.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Computing the power function (optional reading)}{52}{section.3.4}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Stopping rules}{54}{section.3.5}}
\citation{pocock2013clinical}
\citation{VasishthMertzenJaegerGelman2018}
\citation{grodner}
\citation{gibson00}
\citation{grodner}
\citation{lewisvasishth:cogsci05}
\citation{EngelmannJaegerVasishthSubmitted2018}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Linear models}{57}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{57}{section.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Example: Subject and object relative clauses}{57}{subsection.4.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{The simple linear model}{58}{section*.2}}
\citation{VasishthMixture2017}
\citation{grodner}
\@writefile{toc}{\contentsline {subsubsection}{Unpacking the linear model output}{63}{section*.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Residuals of the model m0.}}{65}{figure.4.1}}
\newlabel{fig:residuals}{{4.1}{65}{Residuals of the model m0}{figure.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{How the coefficient estimates are computed}{67}{section*.4}}
\newlabel{eq1a}{{4.11}{67}{How the coefficient estimates are computed}{equation.4.1.11}{}}
\newlabel{matrixeq1}{{4.12}{68}{How the coefficient estimates are computed}{equation.4.1.12}{}}
\newlabel{matrixsum}{{4.14}{68}{How the coefficient estimates are computed}{equation.4.1.14}{}}
\newlabel{matrixsum}{{4.15}{68}{How the coefficient estimates are computed}{equation.4.1.15}{}}
\newlabel{matrixsum}{{4.16}{68}{How the coefficient estimates are computed}{equation.4.1.16}{}}
\newlabel{matrixsum}{{4.17}{68}{How the coefficient estimates are computed}{equation.4.1.17}{}}
\newlabel{matrixsum}{{4.18}{69}{How the coefficient estimates are computed}{equation.4.1.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{Contrast coding}{69}{section*.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}True discovery rate (power)}{71}{subsection.4.1.2}}
\citation{VasishthMertzenJaegerGelman2018}
\citation{JaegerEngelmannVasishth2017}
\citation{gelmanhill07}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}A linear model with a continuous predictor}{73}{section.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Does beauty level affect evaluation?}{75}{subsection.4.2.1}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Using fake-data simulation to understand a model}{76}{section.4.3}}
\bibstyle{plain}
\bibdata{/Users/shravanvasishth/Dropbox/Bibliography/bibcleaned}
\bibcite{EngelmannJaegerVasishthSubmitted2018}{1}
\bibcite{gelmanhill07}{2}
\bibcite{gibson00}{3}
\bibcite{grodner}{4}
\bibcite{hoekstra2014robust}{5}
\bibcite{hoenigheisey}{6}
\bibcite{JaegerEngelmannVasishth2017}{7}
\bibcite{kerns}{8}
\bibcite{lewisvasishth:cogsci05}{9}
\bibcite{mooreetal}{10}
\bibcite{pocock2013clinical}{11}
\bibcite{rice1995mathematical}{12}
\bibcite{VasishthMertzenJaegerGelman2018}{13}
\bibcite{VasishthMixture2017}{14}
