Returning to our noise and deg data, one important point we've neglected is that different subjects have different effects of noise and deg. In the linear models above we are ignoring this.

%%xxx

<<>>=
## returning to our noise data (MD497.df):
## here's an important fact about our data:
# different subjects have different means for no.noise and noise
# and different means for the three levels of deg

means.noise<-with(MD497.df,tapply(rt,list(subj,noise),mean))

means.deg<-with(MD497.df,tapply(rt,list(subj,deg),mean))
@

We can view the differential behavior of subjects in a graph (Figures~\ref{fig:noisebysubj} and \ref{fig:degbysubj}).

\begin{figure}[!htbp]
\centering
<<>>=
## We can visualize these differences graphically:

library(lattice)

## noise by subject (data points):
print(xyplot(rt~noise|subj,
        panel=function(x,y,...){panel.xyplot(x,y,type="r")},MD497.df))
@
\caption{Noise effects by subject.}
\label{fig:noisebysubj}
\end{figure}



\begin{figure}[!htbp]
\centering
<<>>=
## same as above, but for deg:
print(xyplot(rt~deg|subj,
        panel=function(x,y,...){panel.xyplot(x,y,type="r")},MD497.df))
@
\caption{Noise effects by subject.}
\label{fig:degbysubj}
\end{figure}



Given these differences between subjects, you could fit a separate linear model for each subject, collect together the intercepts and slopes for each subject, and then check if the intercepts and slopes are significantly different from zero.

Try this for one subject (s1):

<<>>=
## fit a separate linear model for subject s1:
s1data<-subset(MD497.df,subj=="s1")
lm(rt~noise,s1data)
@

Go back and look at the means for s1 for noise and compare them to the coefficients above.

Now we can do this for every one of our 10 subjects. I don't print this result out because it's consume a lot of pages.

<<>>=
## do the same for each subject using a for-loop
subjects<-paste("s",rep(1:10),sep="")
for(i in subjects){
  sdata<-subset(MD497.df,subj==i)
        lm(rt~noise,sdata)
}
@

There is a function in the package \texttt{lme4} that does the above for you: \texttt{lmList}.

<<>>=
## do the same as the above for-loop for each subject in a single shot:
library(lme4)

lmlist.fm1<-lmList(rt~noise|subj,MD497.df)

print(lmlist.fm1$s1)
@


One can plot the individual lines for each subject, as well as the linear model m0's line (this shows how each subject deviates in intercept and slope from the model m0's intercept and slopes).

<<>>=
plot(as.numeric(MD497.df$noise)-1,
     MD497.df$rt,axes=F,
     xlab="noise",ylab="rt")
axis(1,at=c(0,1),
     labels=c("no.noise","noise"))
axis(2)

subjects<-paste("s",1:10,sep="")

for(i in subjects){
abline(lmlist.fm1[[i]])
}

abline(lm(rt~noise,MD497.df),lwd=3,col="red")
@

To find out if there is an effect of noise, you can simply check whether the slopes of the individual subjects' fitted lines taken together are significantly different from zero:

<<>>=
## now you can test with a t.test whether each coefficient is significantly different from 0:
t.test(coef(lmlist.fm1)[2])
@


The above is called repeated measures regression (see \ref{lm90} for details). We now transition to the next stage of multiple regression: the linear mixed model.

\section{Linear mixed model}

The \textbf{linear mixed model} does something related to the above by-subject fits, but with some crucial twists, as we see below. In the model below, the 
the statement (1$\mid$subj) means that the variance associated with subject intercepts should be estimated, and from that variance the intercepts for each subject should be predicted. 

<<>>=
## the following command fits a linear model, but in addition estimates between-subject variance:

summary(m0.lmer<-lmer(rt~noise+(1|subj),MD497.df))
@


One thing to notice is that the coefficients of the fixed effects of the above model are identical to those in the linear model m0 above. 
The predicted varying intercepts for each subject can be viewed by typing:

<<>>=
ranef(m0.lmer)
@

Or you can display them graphically.


<<>>=
print(dotplot(ranef(m0.lmer,postVar=TRUE)))
@


The model m0.lmer above prints out the following type of linear model:

\begin{equation} \label{eqlmer}
Y_i = \hat{\beta}_{0} + \hat{\beta}_{1}X_i + b_i + \epsilon_i 
\end{equation}

It's just like our linear model except that there are different \textit{predicted} (cf.\ the lmlist function above, where they are \textit{estimated} for each subject) intercepts $b_i$ for each subject. These are assumed by lmer to come from a normal distribution centered around 0; see \cite{gelmanhill07} for more. The ordinary linear model m0 has one intercept $\beta_0$ for all subjects, whereas the linear mixed model with varying intercepts m0.lmer has a different intercept ($\beta_0 + b_i$) for each subject.

We can visualize these different intercepts for each subject as shown below.

<<>>=
(a<-fixef(m0.lmer)[1])
(newa<-a+ranef(m0.lmer)$subj)

ab<-data.frame(newa=newa,b=fixef(m0.lmer)[2])

plot(as.numeric(MD497.df$noise)-1,MD497.df$rt,xlab="noise",ylab="rt",axes=F)
axis(1,at=c(0,1),labels=c("no.noise","noise"))
axis(2)

for(i in 1:10){
abline(a=ab[i,1],b=ab[i,2])
}

abline(lm(rt~noise,MD497.df),lwd=3,col="red")
@

Note that, unlike the figure associated with the lmlist.fm1 model above, which also involves fitting separate models for each subject, the model m0.lmer assumes different intercepts for each subject \textbf{but the same slope}. We can have lmer fit different intercepts AND slopes for each subject:

<<>>=
summary(m1.lmer<-lmer(rt~noise+(1+noise|subj),MD497.df))
@

These fits for each subject are visualized below (the red line shows the model with a single intercept and slope, i.e., our old model m0):

<<>>=
(a<-fixef(m1.lmer)[1])
(b<-fixef(m1.lmer)[2])

(newa<-a+ranef(m1.lmer)$subj[1])
(newb<-b+ranef(m1.lmer)$subj[2])

ab<-data.frame(newa=newa,b=newb)

plot(as.numeric(MD497.df$noise)-1,MD497.df$rt,xlab="noise",ylab="rt",axes=F,
main="varying intercepts and slopes for each subject")
axis(1,at=c(0,1),labels=c("no.noise","noise"))
axis(2)

for(i in 1:10){
abline(a=ab[i,1],b=ab[i,2])
}

abline(lm(rt~noise,MD497.df),lwd=3,col="red")
@

Compare this model with the lmlist.fm1 model we fitted earlier:

<<echo=FALSE,fig.width=6>>=
multiplot <- function(row,col){
     par(mfrow=c(row,col),pty="s")
   }

multiplot(1,2)

plot(as.numeric(MD497.df$noise)-1,MD497.df$rt,axes=F,xlab="noise",ylab="rt",main="ordinary linear model")
axis(1,at=c(0,1),labels=c("no.noise","noise"))
axis(2)

subjects<-paste("s",1:10,sep="")

for(i in subjects){
abline(lmlist.fm1[[i]])
}

abline(lm(rt~noise,MD497.df),lwd=3,col="red")

(a<-fixef(m1.lmer)[1])
(b<-fixef(m1.lmer)[2])

(newa<-a+ranef(m1.lmer)$subj[1])
(newb<-b+ranef(m1.lmer)$subj[2])

ab<-data.frame(newa=newa,b=newb)

plot(as.numeric(MD497.df$noise)-1,MD497.df$rt,axes=F,
main="varying intercepts and slopes",xlab="noise",ylab="rt")
axis(1,at=c(0,1),labels=c("no.noise","noise"))
axis(2)

for(i in 1:10){
abline(a=ab[i,1],b=ab[i,2])
}

abline(lm(rt~noise,MD497.df),lwd=3,col="red")
@

%%% zzz


The above graphic shows some crucial difference between the lmlist (repeated measures) model and the lmer model. Note that the fitted line for each subject in the lmer model is much closer to the m0 model's fitted (red) line. This is because lmlist uses each subject's data separately (resulting in possibly wildly different models, depending on the variability between subjects), whereas lmer ``borrows strength from the mean'' and pushes (or ``shrinks'') the estimated intercepts and slopes of each subject closer to the mean intercepts and slopes (the model m0's intercepts and slopes). Because it shrinks the coefficients towards the means, this is called shrinkage. This is particularly useful when several data points are missing in a particular condition for a particular subject: in an ordinary linear model, estimating coefficients using lmList would lead to very poor estimates for that subject; by contrast, lmer assumes that the estimates for such a subject are not reliable and therefore shrinks that subject's estimate to the mean values.

To see an example of shrinkage, consider the case where we remove three of the data points from subject s8, resulting in exaggeratedly high means for that subject.

First, we read in a data frame which is just the same as MD497.df, except that subject 8 (s8) has only three data points, not six (I took out three of s8's low measures). This skews the subject's estimates for intercept and slope in the lmlist model fit.

<<>>=
MD497.df2<-read.table("data/MD497df.txt",header=T)
@

Next, let's confirm that the new data frame has extreme means for s8:

<<>>=
with(MD497.df,tapply(rt,list(subj,noise),mean,na.rm=TRUE))

with(MD497.df2,tapply(rt,list(subj,noise),mean,na.rm=TRUE))
@

We now fit the lmlist model and the linear mixed model.

<<>>=
lmlist.fm2<-lmList(rt~noise|subj,MD497.df2)

summary(m2.lmer<-lmer(rt~noise+(1+noise|subj),MD497.df2))
@

Now if we plot the model for s8, we find that the lmlist model indeed estimates pretty extreme intercepts for s8. But the linear mixed model predicts an intercept that's much closer to the mean (the red line). Let's just plot s8's fitted line in both models relative to the linear model fitted line.

<<>>=

multiplot <- function(row,col){
     par(mfrow=c(row,col),pty="s")
   }

multiplot(2,2)

## reduced data:
plot(as.numeric(MD497.df2$noise)-1,MD497.df2$rt,axes=F,xlab="noise",ylab="rt",main="ordinary linear model",sub="s8, missing data")
axis(1,at=c(0,1),labels=c("no.noise","noise"))
axis(2)

abline(lmlist.fm2$s8)

abline(lm(rt~noise,MD497.df2),lwd=3,col="red")

(a<-fixef(m2.lmer)[1])
(b<-fixef(m2.lmer)[2])

(newa<-a+ranef(m2.lmer)$subj[1])
(newb<-b+ranef(m2.lmer)$subj[2])

ab<-data.frame(newa=newa,b=newb)

plot(as.numeric(MD497.df2$noise)-1,MD497.df2$rt,axes=F,
main="varying intercepts and slopes",
sub="s8, missing data",
xlab="noise",ylab="rt")
axis(1,at=c(0,1),labels=c("no.noise","noise"))
axis(2)


abline(a=ab[9,1],b=ab[9,2])

abline(lm(rt~noise,MD497.df2),lwd=3,col="red")

## unreduced

plot(as.numeric(MD497.df$noise)-1,MD497.df$rt,axes=F,xlab="noise",ylab="rt",main="ordinary linear model",
,sub="s8, no missing data")
axis(1,at=c(0,1),labels=c("no.noise","noise"))
axis(2)

abline(lmlist.fm1$s8)

abline(lm(rt~noise,MD497.df),lwd=3,col="red")

(a<-fixef(m2.lmer)[1])
(b<-fixef(m2.lmer)[2])

(newa<-a+ranef(m1.lmer)$subj[1])
(newb<-b+ranef(m1.lmer)$subj[2])

ab<-data.frame(newa=newa,b=newb)

plot(as.numeric(MD497.df$noise)-1,MD497.df$rt,axes=F,
main="varying intercepts and slopes",sub="s8, no missing data",xlab="noise",ylab="rt")
axis(1,at=c(0,1),labels=c("no.noise","noise"))
axis(2)


abline(a=ab[9,1],b=ab[9,2])

abline(lm(rt~noise,MD497.df),lwd=3,col="red")
@

One crucial difference between the lmlist model and the lmer model is that the former estimates the parameters for each subject separately, whereas the latter estimates the variance associated with subjects' intercepts (and slopes, if you specify in the model that one should do that) and then \textit{predicts} each subjects intercepts and slopes based on that variance. 

\section{Contrast coding}

Instead of working with the degree and noise data, we will work with the lexical decision data from the languageR package by Harald Baayen. His book \cite{baayenbook} is an excellent one for psycholinguists.

\subsection{Treatment contrasts}

Consider the simplest case where we need to compare reaction times in an experiment involving two conditions.  As mentioned above, we take the lexical decision dataset \texttt{lexdec} from the library \texttt{languageR} as an example.

<<>>=
library(languageR)

## isolate relevant columns
head(lexdec[,c(1,2,5)])
@

This dataset shows log lexical decision times of participants to different words. 

Suppose we want to know whether being a native speaker of English affects reaction time. Before even doing the experiment, it is clear that we would expect that native speakers to have shorter reaction times.  We can verify that the means do have the expected difference; the question is whether this difference is statistically significant:

<<>>=
means.lexdec<-with(lexdec,tapply(RT,NativeLanguage,mean))
@

The mean for English is \Sexpr{round(means.lexdec[1],digits=3)}, and
the means for the other language is
\Sexpr{round(means.lexdec[2],digits=3)}; the difference between the
two is \Sexpr{round(means.lexdec[2] -means.lexdec[1],digits=3)}. These
values become relevant in a moment (recall the discussion of the noise and deg data above, though; it should be clear to you why the means are relevant).

It is straightforward to carry out the comparison between these means
using a linear model. 

<<>>=
summary(lm(RT~NativeLanguage,lexdec))
@

What is the interpretation
of the coefficients?  Comparing the means for each condition with the
coefficients reveals that (i) the intercept's value is
the mean for English; and (ii) the slope's value is the
difference between the two conditions' mean. 

But how does R deliver these particular values for the intercept and
slope? This comes from the contrast coding specified for the predictor
variable. By default, R assigns the so-called \textsc{treatment
  contrast coding} to the predictors: the alphabetically earlier
predictor level (here, English) is coded as 0 (the baseline), and the
other level (here, Other) is coded as 1.  

The interpretation for the
intercept and slope 
derives from this numerical coding: when the predictor is 0 (i.e.,
when the participant is a native speaker of English), the predicted
reaction time is the estimated intercept. When the predictor is coded
as 1 (i.e., the participant is a non-native English speaker), then the
predicted reaction time is the sum of the intercept and the slope. It
is possible to examine the contrast coding using the
\texttt{contrasts} command:

<<>>=
contrasts(lexdec$NativeLanguage)
@ 

As mentioned above, R alphabetically orders the factors and takes the
first condition as the baseline. It is of course possible to take the other level as the baseline:

<<>>=

lexdec$NativeLanguage<-factor(lexdec$NativeLanguage,levels=c("Other","English"))

contrasts(lexdec$NativeLanguage)
@ 


\noindent
Now, the intercept and slope will have a different interpretation:

<<>>=
summary(lm(RT~NativeLanguage,lexdec))
@ 

The intercept now represents the mean score of the level Other, and
the slope the difference between the English and Other scores.  The
sign of the slope is negative because now the difference is computed
by subtracting the mean English score from the mean Other score.

\subsection{Sum contrasts}

Treatment contrasts are only one option, however. It is possible to
utilize the so-called \textsc{sum contrast}, which codes one of the two
conditions as -1 and the other as 1, effectively `centering' the
predictor.

<<>>=
c.sum<-contr.sum(2)
@ 

In our example, we can assign the sum contrast so that Other is 1 and
English is -1 (note that reordering the factors would give the
opposite coding):

<<>>=
contrasts(lexdec$NativeLanguage) <- c.sum
@ 

The linear model's estimated coefficients now look different again:

<<>>=
summary(lm(RT~NativeLanguage,lexdec))
@ 

The intercept is now the grand mean of the two conditions: 

<<>>=
mean(means.lexdec)
@ 

When the predictor is coded as 1 (i.e., when the participant belongs
to the group Other), the predicted RT is 6.39622+0.07791, and when the
predictor is coded as -1 (i.e., when the participang belongs to the
English group), the predicted RT is 6.39622-0.07791.

To summarize, treatment contrasts and sum contrasts are two possible
ways to compare the two conditions, and they answer different
research questions. Treatment contrasts compare one or more condition's mean
against a baseline condition  (we show an example below where more than
two conditions are involved), whereas sum contrasts allow us to
determine whether a condition's mean is significantly different from
the grand mean.

Let us now look at some other contrast coding schemes.

\subsection{Sliding contrasts}

As an illustration, we take the same \texttt{lexdec} dataset and investigate the question:
does word frequency affect reaction time? Here, we would expect
that lower frequency would result in longer reaction time. In the
\texttt{lexdec} dataset, frequency is provided as a continuous variable
(each word has a frequency value associated with it). 
We could fit a linear model where we use continuous frequency values as a predictor of reaction times.
Since our
immediate focus is on qualitative predictors, we
first convert this continuous predictor to a qualitative one: low,
medium and high frequency:

<<>>=
library(gtools)
Freq <-cut(lexdec$Frequency,breaks=3,labels=c("low","med","high"))

lexdec$Freq <- factor(Freq)
@ 

Let us first calculate the mean scores for each level of \texttt{Freq}:

<<>>=
means.freq <- with(lexdec,tapply(RT,Freq,mean))
@ 

The default coding for such a three-condition case is the treatment contrast:

<<>>=
contrasts(lexdec$Freq)
@ 


Suppose we want to know whether frequency
level \texttt{low} leads to significantly longer reaction times than
frequency level \texttt{medium}, and whether 
 frequency
level \texttt{medium} leads to significantly longer reaction times than
frequency level \texttt{high}. R has a contrast coding for answering
this question: \textsc{sliding contrasts} or \textsc{repeated contrasts}:

<<>>=
library(MASS)
c.sliding <- contr.sdif(3)
@ 

The two pairs of means being compared are:

<<>>=
means.freq[2]-means.freq[1]
means.freq[3]-means.freq[2]
@ 

The linear (mixed) model with sliding contrasts yields these means as coefficients:

<<>>=
contrasts(lexdec$Freq) <- c.sliding
summary(lm(RT~Freq,lexdec))
@ 

This contrast coding answers the research question directly: each of
the two differences is significantly different from 0.

Suppose now that our research question had been: is the mean of the
last condition (\texttt{high}), significantly different from the
average of the other two; and are the other two significantly
different from each other? This question can be answered using Helmert contrasts:

<<>>=
c.helmert <- contr.helmert(3)
contrasts(lexdec$Freq) <- c.helmert
contrasts(lexdec$Freq)
@ 

Now we expect to see the following means being compared:

<<>>=
c(means.freq[2]-means.freq[1], means.freq[3]-(means.freq[2]+means.freq[1])/2)
means.freq[3]-(means.freq[1]+means.freq[2])/2
means.freq[2]-means.freq[1]
@ 


The linear model directly compares these means:

<<>>=
summary(lm(RT~Freq,lexdec))
@ 

\noindent
However, note that the coefficients do not match the differences
between means that we just explored.  In order to get the
comparisons of interest, we must take the generalized inverse of a
normalized contrast specification:

<<>>=
c.helmert2 <- matrix(c(  -1,    1,    0,
                   -1/2, -1/2,    1
	                 ),  3,  2, 
	                dimnames=list(c("low", "med", "high"), 
	                              c(".low-med", ".med-high")))

contrasts(lexdec$Freq) <- t(ginv(c.helmert2))

summary(lm(RT~Freq,lexdec))
@ 

Now the coefficients match the mean differences:

<<>>=
means.freq[3]-(means.freq[1]+means.freq[2])/2
means.freq[2]-means.freq[1]
@

The details regarding why we must take the inverse are not important right now, but \cite{venablesripley} has more detail.

\subsection{ANOVA contrast coding}

One can also do a classical anova contrast coding (main effects and interaction). 
Consider a $2\times 2$ design like this data (this is real EEG data from my lab):

<<>>=
data <- read.table("data/mean_600_750.tab",header=T)

head(xtabs(~subj+cond,data))

## conditions:
## 101: P S gram
## 102: S P ungram with intruder
## 103: S S ungram w/o intruder
## 104: P P gram with intruder

#        1  2  3  4
#gram    1 -1 -1  1
#intr.g -1  0  0  1
#intr.u  0 -1  1  0 

head(data)

data$cond<-factor(data$cond,levels=c(101,102,104,103))

## critical channels
critc <- c("F3","FZ","F4","C3","CZ","C4","P3","PZ","P4")

## frontals:
frontc <- c("F3","FZ","F4")

## central:
centralc <- c("C3","CZ","C4")

# posterior:
postc <- c("P3","PZ","P4")

d <- subset(data,chan%in%critc)

library(lme4)

contrasts(d$cond)

anova.contrast <- matrix(c(  -1/2, -1/2, +1/2, +1/2,              # Main effect A
	                 -1/2, +1/2, -1/2, +1/2,              # Main effect B
	                 +1/2, -1/2, -1/2, +1/2),  4,  3,     # Interaction A x B
	                  dimnames=list(c("101", "102", "104", "103"), 
	                                c(".A", ".B", ".AxB"))) 



contrasts(d$cond)<-anova.contrast

contrasts(d$cond)

(fm1 <- lmer(value~cond+(1|subj), d ) )
@

\subsection{Steps in fitting a linear (mixed) model}

Here is a checklist for fitting linear models:

\begin{enumerate}
\item First, check that your data have been correctly extracted. This step is often skipped, and it often leads to mistakes. Did all subjects deliver the expected number of data points? Do you have as many rows in your data frame as you'd expect? Are all items present in each subject's data? Are there any strange values for dependent measures? In other words, carefully check your assumptions about the data before you do anything else.
\item Next, define your contrast coding based on your predictions.  
\item Having fit your model, check your assumptions, such as whether the residuals are approximately normally distributed. Although books like \cite{gelmanhill07} say that the normality of residuals assumption in linear models is the ``least important'' of the assumptions in a linear model, it does not follow (and Gelman would agree) that you can simply ignore the normality of residuals assumption. This is especially important when, as is common in psycholinguistics, we want to do a hypothesis test. I explain this point next. The text below is taken almost verbatim from a comment I made on Andrew Gelman's blog.

\begin{quote}
Suppose we are interested in null hypothesis tests in linear models, e.g., $H_0: \beta_1 = 0$, where $\beta_0$ is one of the parameters in the model. Suppose also that we have a ``lot'' of data. To make things concrete, assume that we have a 2×2 within subjects design, with 100 subjects; each subject sees one of the four conditions in the 2×2 design 24 times (the standard counterbalancing done in psychology). So, each subject will see each condition 24 times. Assume that the dependent measure is something like reading times.
Linear mixed models are a standard way to analyze such data.

Here is the argument (it's a bit technical but I will elaborate on it in class) that suggests that checking the normality assumption of residuals is necessary.
Note that $\hat{\beta} \sim N_p (\beta,\sigma^2 (X^T X)^{-1})$, and that
$\frac{\hat{\sigma}^2}{\sigma^2} \sim \frac{\chi^2_{n-p}}{n-p}$.

From distributional theory we know that $T=\frac{X}{\sqrt{Y/v}}$, when $X\sim N(0,1)$ and $Y\sim \chi^2_{v}$.

Let
$x_i$ be a column vector containing the values of the explanatory/regressor variables for a new observation $i$. Then if we define:

\begin{equation}
X=\frac{x_i^T \hat{\beta} – x_i^T \beta}{\sqrt{\sigma^2 x_i^T (X^T X)^{-1}x_i}} \sim N(0,1)
\end{equation}

and

\begin{equation}
Y=\frac{\hat{\sigma}^2}{\sigma^2} \sim \frac{\chi^2_{n-p}}{n-p}
\end{equation}

it follows that $T=\frac{X}{\sqrt{Y/v}}$:

\begin{equation}
T= \frac{x_i^T \hat{\beta} – x_i^T \beta}{\sqrt{\hat{\sigma}^2 x_i^T (X^T X)^{-1}x_i}} =
\frac{ \frac{x_i^T \hat{\beta} – x_i^T \beta}{\sqrt{\sigma^2 x_i^T (X^T X)^{-1}x_i}}}{\sqrt{\frac{\hat{\sigma}^2}{\sigma^2}}}
\sim t_{n-p}
\end{equation}

I.e., a 95\% CI:

\begin{equation}
x_i^T \hat{\beta} \pm t_{n-p,1-\alpha/2}\sqrt{\hat{\sigma}^2 x_i^T(X^T X)^{-1}x_i}
\end{equation}

So, although we can estimate $\hat{\beta}$ without any distributional assumptions, we cannot calculate confidence intervals for parameters, and we can't do hypothesis testing relating to these parameters using F tests because we don't know that $\hat{\beta}$ is multivariate normal because the distribution of $y$ might not be multivariate normal (because the distribution of $\epsilon$ might not be normal).
\end{quote}

We can investigate the consequences of non-normality of residuals with a simulation.

<<>>=
nsim<-100
n<-100
pred<-rep(c(0,1),each=n/2)
store<-matrix(0,nsim,5)

## should the distribution of errors be non-normal?
non.normal<-TRUE

## true effect:
beta.1<-0.5

for(i in 1:nsim){
## assume non-normality of residuals?
## yes:
if(non.normal==TRUE){
errors<-rchisq(n,df=1)
errors<-errors-mean(errors)} else {
## no:
errors<-rnorm(n)
}
## generate data:
y<-100 + beta.1*pred + errors
fm<-lm(y~pred)
## store coef., SE, t-value, p-value:
store[i,1:4]<-summary(fm)$coefficients[2,c(1,2,3,4)] 
}
@

We can calculate the probability of finding a significant effect given that the null hypothesis is false:

<<>>=
## ``observed'' power for raw scores:
table(store[,4]<0.05)[2]
@

We see that there is a huge loss of power compared to the case where the residuals are normal (exercise). 

Note that the coverage of the 95\% CIs is unaffected, but this is not interesting for us when we are doing hypothesis testing! 

<<>>=
## CIs:
upper<-store[,1]+2*store[,2]
lower<-store[,1]-2*store[,2]
## CIs' coverage is unaffected by skewness:
table(lower<beta.1 & upper>beta.1)
@

Here is the type of residual distribution we have in the above simulation; it is pretty typical for reading and reaction time studies.

<<fig=T,echo=F>>=
## typical shape of residuals in reading studies:
library(car)
qqPlot(residuals(fm))
@

Note also that if the residuals are non-normally distributed, your fitted model itself is no longer realistic for the data. You can establish this by doing what Gelman and Hill suggest we do for evaluating model quality: simulate new data and look at whether these simulated values fall in the right ball-park. (exercise)


\item 
Related to the above point, you should use the boxcox function in the MASS package in R to find out which transform you need to stabilize variance. Examples are provided in the case studies chapter. 
\item After having fit the model, check whether there are influential values. Use the influence.ME package for this purpose. 
\item Finally, displaying the results of a linear mixed model: usually we are not interested in the random effects parameter estimates, only the fixed effects estimates. One can use something like the extractfit function and the myxtable function (below) that prints out the linear mixed model fit as a formatted \LaTeX\ table (this is useful if you are working in \LaTeX, which is often the case in linguistics).

<<>>=
extractfit<-function(mod,indices=2:4,
                     coln=c("coef.","SE","t-value"),
                     fac,dig=2,model.type="LM"){
  if(model.type=="LM"){
    ##LM:
    fixefs<-coef(mod)[indices]} else {
    ##LMER:  
    fixefs<-fixef(mod)[indices]
    }
  SEs<-sqrt(diag(vcov(mod)))[indices]
  torz<-fixefs/SEs
  results<-round(cbind(fixefs,SEs,torz),digits=dig)
  results<-data.frame(fac=fac,results)
  colnames(results)<-coln
  rownames(results)<-NULL
  results
}

myxtable<-function(res,cap,lab){
  print(xtable(res,caption=cap,label=lab),
        include.rownames=F)}
@

Here is an example:

<<>>=
m0<-lm(rt~noise,MD497.df)
results.m0<-extractfit(m0,
                        coln=c("","coef.",
                               "SE","t-value"),
                        indices=1:2,
                        fac=c("Intercept","noise"))

#library(xtable)
@

<<results='asis'>>=
myxtable(results.m0,
         cap="The effect of noise on reaction time.",
         lab="tab:m0")
@

\item The final step is producing a good quality summary plot or plots of the results. I discuss this in the chapter that illustrates model fit using case studies.
\end{enumerate}

\subsection{Where to look for more examples}

See the case studies in the next chapter, and the website: http://openscience.uni-leipzig.de (the Mind Research Repository) for more examples.


